---
title: "Walkthrough: main features"
output: 
  bookdown::html_document2:
    css: "style.css"
    number_sections: yes
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
vignette: >
  %\VignetteIndexEntry{main-features}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r klippy, echo = FALSE}
# Create a button for copy pasting code chunk to clipboard
klippy::klippy(position = c("top", "right"))

```

```{r global_options, include = FALSE}
# Load knitr package to change options
library(knitr)

# Set default options for the chunks
opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    message = FALSE,
    comment = ""
)

# Set the width of the console output to a large number (for sidewise scroll)
options(width = 1e3)

```

```{r setup}
# Load R packages
library(gspcr)      # this package!
library(superpc)    # alternative comparison package
library(patchwork)  # combining ggplots

# Set a seed
set.seed(20230415)

```


Prepare data

```{r}
# Comment goal of code
X <- GSPCRexdata$X$cont
y <- GSPCRexdata$y$cont

```

# Estimation

Run the version `gspcr`:

```{r }
# Train the GSPCR model
out <- cv_gspcr(
  dv = y,
  ivs = X,
  fam = "gaussian",
  nthrs = 20,
  npcs_range = 1:5,
  K = 10,
  fit_measure = "F",
  thrs = "normalized",
  min_features = 1,
  max_features = ncol(X),
  oneSE = TRUE
)

# Use the plotting function
plot_output <- plot(
  x = out,
  y = "F",
  labels = TRUE,
  errorBars = TRUE,
  discretize = FALSE
)

# And plot
plot_output

```

## Compare results with `superpc`

Run `superpc` version

```{r }
# Define a train data
data.train <- list(
  x = t(as.matrix(scale(X))),
  y = y,
  featurenames = colnames(X)
)

# Train the model (computes the scores for each feature)
train.obj <- superpc.train(
  data = data.train,
  type = "regression"
)

# Cross-validate the model
cv.obj <- superpc.cv(
  fit = train.obj,
  data = data.train,
  min.features = 1,
  max.features = nrow(data.train$x),
  n.fold = 10,
  n.threshold = 20,
  n.components = 5
)

# Create plot the cross-validation curves
cv.obj_plot <- superpc.plotcv(cv.obj)

```

The computed thresholds are the same:

```{r}
# Report the threshold values
data.frame(
    superpc = cv.obj$thresholds,
    gpscr = out$thr,
    diff = cv.obj$thresholds - out$thr
)

```

# Association measures

We can run it using the generalized R2 and the log-likelihoods of simple GLMs as a measure of association.

```{r }
#| label: fig-association-measures
#| fig-cap: "Solution paths for different association measures."
#| fig-width: 9
#| fig-height: 3

# Measures
trhs_vec <- c("normalized", "PR2", "LLS")

# Train the GSPCR model with the different values
out_trhs <- lapply(trhs_vec, function(i) {
    cv_gspcr(
        dv = y,
        ivs = X,
        fam = "gaussian",
        nthrs = 20,
        npcs_range = 1,
        K = 10,
        fit_measure = "F",
        thrs = i,
        min_features = 1,
        max_features = ncol(X),
        oneSE = TRUE
    )
})

# Plot them
plots <- lapply(out_trhs, function(i) {
    plot(
        x = i,
        y = "F",
        labels = FALSE,
        errorBars = FALSE,
        discretize = FALSE,
        print = FALSE
    )
})

# Patchwork ggplots
plots[[1]] + plots[[2]] + plots[[3]]

```

# Fit measures

We can run it using different cross-validation score statistics. For a given number of PCs, equivalent solution paths are drawn by using the different fit measures.

```{r }
# Measures
fit_measure_vec <- c("F", "LRT", "AIC", "BIC", "PR2", "MSE")

# Train the GSPCR model with the different values
out_fit_meas <- lapply(fit_measure_vec, function(i) {
    cv_gspcr(
        dv = y,
        ivs = X,
        fam = "gaussian",
        nthrs = 20,
        npcs_range = 1,
        K = 10,
        fit_measure = i,
        thrs = "normalized",
        min_features = 1,
        max_features = ncol(X),
        oneSE = TRUE
    )
})

# Plot them
plots <- lapply(seq_along(fit_measure_vec), function(i) {
    # Reverse y?
    rev <- grepl("MSE|AIC|BIC", fit_measure_vec[i])

    # Make plots
    plot(
        x = out_fit_meas[[i]],
        y = fit_measure_vec[[i]],
        labels = FALSE,
        y_reverse = rev,
        errorBars = FALSE,
        discretize = FALSE,
        print = FALSE
    )
})

# Patchwork ggplots
(plots[[1]] + plots[[2]] + plots[[6]]) / (plots[[5]] + plots[[3]] + plots[[4]])

```

You can also check out the solutions that are returned by each measure:

```{r }
# Default solutions
res <- sapply(
    1:length(out_fit_meas),
    function(meth) {
        c(
            thr_value = out_fit_meas[[meth]]$thr_cv,
            thr_number = which(out_fit_meas[[meth]]$thr_cv == out_fit_meas[[meth]]$thr),
            Q = out_fit_meas[[meth]]$Q_cv
        )
    }
)

# Give meaningful names
colnames(res) <- fit_measure_vec

# Print rounded results
round(t(res), 3)

```

This is true for any number of pcs:

```{r }
# Measures
fit_measure_vec <- c("F", "LRT", "AIC", "BIC", "PR2", "MSE")

# Train the GSPCR model with the different values
out_fit_meas <- lapply(fit_measure_vec, function(i) {
    cv_gspcr(
        dv = y,
        ivs = X,
        fam = "gaussian",
        nthrs = 20,
        npcs_range = 5,
        K = 10,
        fit_measure = i,
        thrs = "normalized",
        min_features = 1,
        max_features = ncol(X),
        oneSE = TRUE
    )
})

# Plot them
plots <- lapply(seq_along(fit_measure_vec), function(i) {
    # Reverse y?
    rev <- grepl("MSE|AIC|BIC", fit_measure_vec[i])

    # Make plots
    plot(
        x = out_fit_meas[[i]],
        y = fit_measure_vec[[i]],
        labels = FALSE,
        y_reverse = rev,
        errorBars = FALSE,
        discretize = FALSE,
        print = FALSE
    )
})

# Patchwork ggplots
(plots[[1]] + plots[[2]] + plots[[6]]) / (plots[[5]] + plots[[3]] + plots[[4]])

```

The plots show very similar solutions paths across fit measures.
An interesting detail in the plot is that they highlight how for high values of the threshold parameter, only 4, 3, and 2 components could be used.

# Cross-validation of the number of components

```{r }
# Measures
fit_measure_vec <- c("F", "LRT", "AIC", "BIC", "PR2", "MSE")

# Train the GSPCR model with the different values
out_fit_meas <- lapply(fit_measure_vec, function(i) {
    cv_gspcr(
        dv = y,
        ivs = X,
        fam = "gaussian",
        nthrs = 20,
        npcs_range = c(1, 5),
        K = 10,
        fit_measure = i,
        thrs = "normalized",
        min_features = 1,
        max_features = ncol(X),
        oneSE = TRUE
    )
})

# Plot them
plots <- lapply(seq_along(fit_measure_vec), function(i) {
    # Reverse y?
    rev <- grepl("MSE|AIC|BIC", fit_measure_vec[i])

    # Make plots
    plot(
        x = out_fit_meas[[i]],
        y = fit_measure_vec[[i]],
        labels = TRUE,
        y_reverse = rev,
        errorBars = FALSE,
        discretize = FALSE,
        print = FALSE
    )
})

# Patchwork ggplots
(plots[[1]] + plots[[2]] + plots[[6]]) / (plots[[5]] + plots[[3]] + plots[[4]])

```

This highlights the problem of the selection of the number of components.
BIC seems to be the best likelihood-based alternative to the F statistic.

## 1SE solutions

The results for all fit measures except `F` struggle with accounting for measure complexity. The use of a simple 1-standard-error rule helps obviate this problem.
First, fit the models with many possible number of components:

```{r }
# Train the GSPCR model with many number of components
out_fit_meas <- lapply(fit_measure_vec, function(i) {
    cv_gspcr(
        dv = y,
        ivs = X,
        fam = "gaussian",
        nthrs = 10,
        npcs_range = 1:10,
        K = 10,
        fit_measure = i,
        thrs = "normalized",
        min_features = 1,
        max_features = ncol(X),
        oneSE = TRUE
    )
})

# Plot them
plots <- lapply(seq_along(fit_measure_vec), function(i) {
    # Reverse y?
    rev <- grepl("MSE|AIC|BIC", fit_measure_vec[i])

    # Make plots
    plot(
        x = out_fit_meas[[i]],
        y = fit_measure_vec[[i]],
        labels = TRUE,
        y_reverse = rev,
        errorBars = TRUE,
        discretize = FALSE,
        print = FALSE
    )
})

# Patchwork ggplots
(plots[[1]] + plots[[2]] + plots[[6]]) / (plots[[5]] + plots[[3]] + plots[[4]])

```

Then, extract the solutions obtained by each:

```{r }
# Standard solutions
res <- sapply(
    1:length(out_fit_meas),
    function(meth) {
        as.numeric(out_fit_meas[[meth]]$sol_table["standard", ])
    }
)

# Give meaningful names
dimnames(res) <- list(c("thr_value", "thr_number", "Q"), fit_measure_vec)

# Print rounded results
round(t(res), 3)

```

Finally, you can check which solutions would be chosen by using the 1-standard-error rule:

```{r }
# 1se solutions
res_1se <- sapply(
    1:length(out_fit_meas),
    function(meth) {
        as.numeric(out_fit_meas[[meth]]$sol_table["oneSE", ])
    }
)

# Give meaningful names
dimnames(res_1se) <- list(c("thr_value", "thr_number", "Q"), fit_measure_vec)

# Print rounded results
round(t(res_1se), 3)

```

## Is cross-validation working?

Run the method with two simple component choices with and without cross-validation.

```{r }
# Train the GSPCR model with many number of components
out_fit_meas_cv <- lapply(fit_measure_vec, function(i) {
    cv_gspcr(
        dv = y,
        ivs = X,
        fam = "gaussian",
        nthrs = 10,
        npcs_range = c(5, 20),
        K = 10,
        fit_measure = i,
        thrs = "normalized",
        min_features = 1,
        max_features = ncol(X),
        oneSE = TRUE
    )
})

# Plot them
plots <- lapply(seq_along(fit_measure_vec), function(i) {
    # Reverse y?
    rev <- grepl("MSE|AIC|BIC", fit_measure_vec[i])

    # Make plots
    plot(
        x = out_fit_meas_cv[[i]],
        y = fit_measure_vec[[i]],
        labels = TRUE,
        y_reverse = rev,
        errorBars = FALSE,
        discretize = FALSE,
        print = FALSE
    )
})

# Patchwork ggplots
(plots[[1]] + plots[[2]] + plots[[6]]) / (plots[[5]] + plots[[3]] + plots[[4]])

```

```{r }
# Train the GSPCR model with many number of components
out_fit_meas_no_CV <- lapply(fit_measure_vec, function(i) {
    cv_gspcr(
        dv = y,
        ivs = X,
        fam = "gaussian",
        nthrs = 10,
        npcs_range = c(5, 20),
        K = 1,
        fit_measure = i,
        thrs = "normalized",
        min_features = 1,
        max_features = ncol(X),
        oneSE = TRUE
    )
})

# Plot them
plots <- lapply(seq_along(fit_measure_vec), function(i) {
    # Reverse y?
    rev <- grepl("MSE|AIC|BIC", fit_measure_vec[i])

    # Make plots
    plot(
        x = out_fit_meas_no_CV[[i]],
        y = fit_measure_vec[[i]],
        labels = TRUE,
        y_reverse = rev,
        errorBars = TRUE,
        discretize = FALSE,
        print = FALSE
    )
})

# Patchwork ggplots
(plots[[1]] + plots[[2]] + plots[[6]]) / (plots[[5]] + plots[[3]] + plots[[4]])

```

The solution we would have found using CV is:

```{r }
# Standard solutions
res_CV <- sapply(
    1:length(out_fit_meas_cv),
    function(meth) {
        as.numeric(out_fit_meas_cv[[meth]]$sol_table["standard", ])
    }
)

# Give meaningful names
dimnames(res_CV) <- list(c("thr_value", "thr_number", "Q"), fit_measure_vec)

# Print rounded results
round(t(res_CV), 3)

```

We can then extract the solutions that we would obtain with this run of the function:

```{r }
# Standard solutions
res_no_CV <- sapply(
    1:length(out_fit_meas_no_CV),
    function(meth) {
        as.numeric(out_fit_meas_no_CV[[meth]]$sol_table["standard", ])
    }
)

# Give meaningful names
dimnames(res_no_CV) <- list(c("thr_value", "thr_number", "Q"), fit_measure_vec)

# Print rounded results
round(t(res_no_CV), 3)

```

As you can see, using CV we find the same solution no matter what the outcome measure, while without using CV, only the AIC, BIC, and F are able to select a low number of PCs over a high number of PCs.

# Alternatives to CV

To speed up the model-fitting process, it can be a good idea to find model-building strategies that are less time-consuming than CV.
You can use the BIC fit measure without CV to select the appropriate threshold value and the number of components.
To do so, you can specify the number of folds to 0 and the fit measure to BIC or AIC.

```{r }
#| label: fig-no-cv
#| fig-cap: "Solution paths obtained without using cross-validation."
#| fig-width: 9
#| fig-height: 3

# Define vector of measures to be used
fit_measure_vec <- c("LRT", "AIC", "BIC")

# Train the GSPCR model with the different values
out_fit_meas <- lapply(fit_measure_vec, function(i) {
    cv_gspcr(
        dv = y,
        ivs = X,
        fam = "gaussian",
        nthrs = 10,
        npcs_range = c(1, 2, 5, 20),
        K = 1,
        fit_measure = i,
        thrs = "normalized",
        min_features = 1,
        max_features = ncol(X),
        oneSE = TRUE
    )
})

# Plot them
plots <- lapply(seq_along(fit_measure_vec), function(i) {
    # Reverse y?
    rev <- grepl("MSE|AIC|BIC", fit_measure_vec[i])

    # Make plots
    plot(
        x = out_fit_meas[[i]],
        y = fit_measure_vec[[i]],
        labels = TRUE,
        y_reverse = rev,
        errorBars = FALSE,
        discretize = FALSE,
        print = FALSE
    )
})

# Patchwork ggplots
plots[[1]] + plots[[2]] + plots[[3]]

```

You can also look at the solutions:

```{r }
# Put solutions together
rbind(
    LRT = out_fit_meas[[1]]$sol_table["standard", ],
    AIC = out_fit_meas[[2]]$sol_table["standard", ],
    BIC = out_fit_meas[[3]]$sol_table["standard", ]
)

```

# TL;DR, just give me the code!

```{r TLDR, ref.label = knitr::all_labels()[!knitr::all_labels() %in% c("global_options", "klippy")], echo = TRUE, eval = FALSE}
```

<!-- Remove white space generated by toc_float -->
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>